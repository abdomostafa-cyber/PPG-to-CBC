{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "import os\n",
    "from scipy.signal import spectrogram\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet50\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import zipfile\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(repo_location):\n",
    "    # Path to the service account key file\n",
    "    SERVICE_ACCOUNT_FILE = os.path.join(repo_location, 'ppg-ml-427113-ecc7e656b7f5.json')\n",
    "\n",
    "    # Define the required scopes\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "    # Authenticate using the service account\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "    # Build the Drive API client\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    # File ID of the .zip file to download\n",
    "    file_id = '1uMejT3pEJVFKM20bzpsbS35NJSuG85DL'\n",
    "\n",
    "    # Request to download the .zip file\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    zip_file_path = 'MIMIC CSVs.zip'\n",
    "\n",
    "    with io.FileIO(zip_file_path, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "\n",
    "    # Extract the downloaded .zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(repo_location, \"CSVs\"))  # Specify the directory to extract to\n",
    "\n",
    "    #Delete the zip file\n",
    "    os.remove(zip_file_path)\n",
    "    print(\"File downloaded and extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_file(file_path):\n",
    "    def recursively_load_data(h5_obj):\n",
    "        if isinstance(h5_obj, h5py.Dataset):\n",
    "            data = h5_obj[()]\n",
    "            if isinstance(data, bytes):  # Decode byte strings\n",
    "                return data.decode()\n",
    "            elif isinstance(data, np.ndarray) and data.dtype.type is np.bytes_:\n",
    "                return data.astype(str)  # Decode byte strings in numpy arrays\n",
    "            return data\n",
    "        elif isinstance(h5_obj, h5py.Group):\n",
    "            data = {}\n",
    "            for key, item in h5_obj.items():\n",
    "                data[key] = recursively_load_data(item)\n",
    "            return data\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported type: {type(h5_obj)}\")\n",
    "\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        return recursively_load_data(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_to_datetime(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format='%Y-%m-%d-%H-%M-%S')\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date '{date_str}': {e}\")\n",
    "        return pd.NaT\n",
    "\n",
    "def process_file(path, file, subject_id, interval, labs_filtered):\n",
    "    data = load_h5_file(os.path.join(path, file))\n",
    "\n",
    "    if ('Segment_Time') in data['Subj_Wins'].keys():\n",
    "\n",
    "        # Convert the data file into a dataframe \n",
    "        df_data = pd.DataFrame(data['Subj_Wins']['PPG_Raw'].squeeze())\n",
    "\n",
    "        # Insert the Segment Time variable to the dataframe\n",
    "        df_data.insert(0, 'Segment_Time', data['Subj_Wins']['Segment_Time'])\n",
    "\n",
    "        # Insert lab_flag variable to the dataframe. All values are equal to nan for now\n",
    "        df_data.insert(1, 'lab_flag', np.nan)\n",
    "\n",
    "\n",
    "        # Insert SubjectID, Age and gender variables to the dataframe\n",
    "        df_data.insert(1, 'SubjectID', subject_id)\n",
    "        df_data.insert(2, 'Age', data['Subj_Wins']['Age'])\n",
    "        df_data.insert(3, 'Gender', data['Subj_Wins']['Gender'])\n",
    "\n",
    "        #Normalize the age variable by 130 to get a value between 0 and 1\n",
    "        df_data['Age'] = df_data['Age']/130\n",
    "\n",
    "        # Convert numpy.str_ to native Python strings and convert to datetime\n",
    "        df_data['Segment_Time'] = df_data['Segment_Time'].apply(lambda x: x[0] if isinstance(x, np.ndarray) else x).astype(str)\n",
    "        df_data['Segment_Time'] = df_data['Segment_Time'].apply(safe_to_datetime)\n",
    "                \n",
    "        mask = (labs_filtered.SUBJECT_ID == subject_id)\n",
    "        labs_filtered = labs_filtered[mask].reset_index(drop=True)\n",
    "        \n",
    "        matching_labs_indices = set()\n",
    "        matching_data_indices = set()\n",
    "\n",
    "        start_time = df_data.Segment_Time - interval\n",
    "        end_time = df_data.Segment_Time + interval\n",
    "\n",
    "        for i, row in df_data.iterrows():\n",
    "            matching_labs_indices.update(\n",
    "                labs_filtered[(labs_filtered.CHARTTIME >= start_time[i]) & (labs_filtered.CHARTTIME <= end_time[i])].index\n",
    "            )\n",
    "\n",
    "        lab_start_time = labs_filtered.CHARTTIME - interval\n",
    "        lab_end_time = labs_filtered.CHARTTIME + interval\n",
    "\n",
    "        for i, row in labs_filtered.iterrows():\n",
    "            matching_data_indices.update(\n",
    "                df_data[(df_data.Segment_Time >= lab_start_time[i]) & (df_data.Segment_Time <= lab_end_time[i])].index\n",
    "            )\n",
    "\n",
    "        # Convert sets to lists for indexing\n",
    "        matching_labs_indices = list(matching_labs_indices)\n",
    "        matching_data_indices = list(matching_data_indices)\n",
    "\n",
    "        df_data = df_data.loc[matching_data_indices].reset_index(drop=True)\n",
    "        labs_filtered = labs_filtered.loc[matching_labs_indices].reset_index(drop=True)\n",
    "\n",
    "        start_time = df_data.Segment_Time - interval\n",
    "        end_time = df_data.Segment_Time + interval\n",
    "\n",
    "        # Vectorize the process of creating lab flags\n",
    "        for i in range(len(df_data)):\n",
    "            labs_subset = labs_filtered[\n",
    "                (labs_filtered.CHARTTIME >= start_time[i]) &\n",
    "                (labs_filtered.CHARTTIME <= end_time[i])\n",
    "            ]\n",
    "            if labs_subset.shape[0] > 0:\n",
    "                flag_sum = labs_subset.FLAG.sum()\n",
    "                df_data.loc[i, 'lab_flag'] = 1 if flag_sum > 0 else 0 if flag_sum == 0 else np.nan\n",
    "\n",
    "            # Check if we have reached 100 samples for either flag, and filter out the other flag once it happens\n",
    "            if df_data[df_data.lab_flag == 0].shape[0] == 100:\n",
    "                labs_subset = labs_subset[labs_subset.FLAG == 1]\n",
    "            if df_data[df_data.lab_flag == 1].shape[0] == 100:\n",
    "                labs_subset = labs_subset[labs_subset.FLAG == 0]\n",
    "        \n",
    "        return df_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_optimized(path, file, subject_id, interval, labs_filtered):\n",
    "    data = load_h5_file(os.path.join(path, file))\n",
    "\n",
    "    if ('Segment_Time') in data['Subj_Wins'].keys():\n",
    "\n",
    "        # Convert the data file into a dataframe \n",
    "        df_data = pd.DataFrame(data['Subj_Wins']['PPG_Raw'].squeeze())\n",
    "\n",
    "        # Insert the Segment Time variable to the dataframe\n",
    "        df_data.insert(0, 'Segment_Time', data['Subj_Wins']['Segment_Time'])\n",
    "\n",
    "        # Insert lab_flag variable to the dataframe. All values are equal to nan for now\n",
    "        df_data.insert(1, 'lab_flag', np.nan)\n",
    "\n",
    "        # Insert SubjectID, Age and Gender variables to the dataframe\n",
    "        df_data.insert(1, 'SubjectID', subject_id)\n",
    "        df_data.insert(2, 'Age', data['Subj_Wins']['Age'])\n",
    "        df_data.insert(3, 'Gender', data['Subj_Wins']['Gender'])\n",
    "\n",
    "        # Normalize the age variable by 130 to get a value between 0 and 1\n",
    "        df_data['Age'] = df_data['Age']/130\n",
    "\n",
    "        # Convert numpy.str_ to native Python strings and convert to datetime\n",
    "        df_data['Segment_Time'] = df_data['Segment_Time'].apply(lambda x: x[0] if isinstance(x, np.ndarray) else x).astype(str)\n",
    "        df_data['Segment_Time'] = df_data['Segment_Time'].apply(safe_to_datetime)\n",
    "   \n",
    "        # Filter labs_filtered for the specific subject\n",
    "        labs_filtered = labs_filtered[labs_filtered.SUBJECT_ID == subject_id]\n",
    "\n",
    "        # Create time windows\n",
    "        df_data['start_time'] = df_data['Segment_Time'] - interval\n",
    "        df_data['end_time'] = df_data['Segment_Time'] + interval\n",
    "\n",
    "        labs_filtered['start_time'] = labs_filtered['CHARTTIME'] - interval\n",
    "        labs_filtered['end_time'] = labs_filtered['CHARTTIME'] + interval\n",
    "\n",
    "        # Merge df_data and labs_filtered based on time windows\n",
    "        merged = pd.merge_asof(df_data.sort_values('Segment_Time'), \n",
    "                            labs_filtered.sort_values('CHARTTIME'), \n",
    "                            left_on='Segment_Time', right_on='CHARTTIME', \n",
    "                            direction='nearest', tolerance=interval)\n",
    "        \n",
    "        # Assign lab_flag based on FLAG\n",
    "        merged['lab_flag'] = np.where(merged['FLAG'].notna(), merged['FLAG'], np.nan)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        merged.drop(columns=['start_time_y', 'end_time_y', 'CHARTTIME', 'SUBJECT_ID',\n",
    "                             'FLAG', 'start_time_x', 'end_time_x', 'HADM_ID', 'ROW_ID',\n",
    "                             'ITEMID', 'VALUE', 'VALUENUM', 'VALUEUOM', 'Panel', 'Subpanel'], inplace=True)\n",
    "\n",
    "        return merged.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.y_data = df.lab_flag.values\n",
    "        self.X_data = df.loc[:, df.columns.intersection([str(i) for i in range(0, 1250)])]\n",
    "        if ('Age' in self.df.columns) and ('Gender' in self.df.columns):\n",
    "            self.age_data = df.Age.values\n",
    "            # Change gender values from 'M' and 'F' to 0 and 1\n",
    "            self.gender_data = df.Gender.map({'M': 0, 'F': 1}).values\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X_data.iloc[idx]\n",
    "        _, _, spec = spectrogram(row.values.astype(np.float32), fs=125, nperseg=256)\n",
    "        y = self.y_data[idx]\n",
    "        if ('Age' in self.df.columns) and ('Gender' in self.df.columns):\n",
    "            age = torch.tensor(self.age_data[idx], dtype=torch.float32)\n",
    "            gender = torch.tensor(self.gender_data[idx], dtype=torch.float32)\n",
    "            X = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension for grayscale image\n",
    "            return X, age, gender, y\n",
    "        else:\n",
    "            X = torch.tensor(spec, dtype=torch.float32)\n",
    "            return X, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWaveletDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.y_data = df.lab_flag.values\n",
    "        self.X_data = df.loc[:, df.columns.intersection([str(i) for i in range(0, 1250)])]\n",
    "        if ('Age' in self.df.columns) and ('Gender' in self.df.columns):\n",
    "            self.age_data = df.Age.values\n",
    "            # Change gender values from 'M' and 'F' to 0 and 1\n",
    "            self.gender_data = df.Gender.map({'M': 0, 'F': 1}).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X_data.iloc[idx]\n",
    "        signal = row.values.astype(np.float32)\n",
    "\n",
    "        # Perform Discrete Wavelet Transform\n",
    "        coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "        coeffs_array = np.concatenate(coeffs)\n",
    "\n",
    "        y = self.y_data[idx]\n",
    "        if ('Age' in self.df.columns) and ('Gender' in self.df.columns):\n",
    "            age = torch.tensor(self.age_data[idx], dtype=torch.float32)\n",
    "            gender = torch.tensor(self.gender_data[idx], dtype=torch.float32)\n",
    "            X = torch.tensor(coeffs_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add channel dimension if needed\n",
    "            return X, age, gender, y\n",
    "        else:\n",
    "            X = torch.tensor(coeffs_array, dtype=torch.float32).unsqueeze(0)\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optim, criterion, epoch, device, gender_age_used, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in trainloader:\n",
    "        if gender_age_used == \"GA_used\":\n",
    "            inputs, age, gender, labels = data\n",
    "            inputs, age, gender, labels = inputs.to(device), age.to(device), gender.to(device), labels.to(device)\n",
    "            labels = labels.long()  # Convert labels to Long type\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs, age, gender)\n",
    "        else:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "        # print(inputs.shape)\n",
    "        # print(outputs.shape, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_acc = correct / total\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    # print(f\"Epoch {epoch} loss: {train_loss}\")\n",
    "    # print(f\"Epoch {epoch} accuracy: {train_acc}\")\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(model, testloader, criterion, best_val_acc, device, test, repo_location, gender_age_used):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            \n",
    "            if gender_age_used == \"GA_used\":\n",
    "                inputs, age, gender, labels = data\n",
    "                inputs, age, gender, labels = inputs.to(device), age.to(device), gender.to(device), labels.to(device)\n",
    "                labels = labels.long()  # Convert labels to Long type\n",
    "                outputs = model(inputs, age, gender)\n",
    "            else:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                labels = labels.long()\n",
    "                outputs = model(inputs.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = (running_loss / len(testloader))\n",
    "    val_acc = (correct / total)      \n",
    "\n",
    "    # print(f\"Validation loss: {val_loss}\")\n",
    "    # print(f\"Validation accuracy: {val_acc}\")\n",
    "\n",
    "    if correct / total > best_val_acc:\n",
    "        best_val_acc = correct / total\n",
    "        torch.save(model.state_dict(), os.path.join(repo_location, f\"models/bestmodel_{test}_{gender_age_used}.pth\"))\n",
    "    return val_loss, val_acc, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc, test, gender_age_used=\"\"):\n",
    "    epochs = len(train_loss)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "    ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'Epoch vs Loss for {test}')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "    ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(f'Epoch vs Accuracy for {test}')\n",
    "    ax2.legend()\n",
    "    fig.set_size_inches(15.5, 5.5)\n",
    "    fig.savefig(f\"plots/{test}{gender_age_used}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simedy2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
